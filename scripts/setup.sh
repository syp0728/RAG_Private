#!/bin/bash
# Private RAG ì‹œìŠ¤í…œ ì´ˆê¸° ì„¤ì • ìŠ¤í¬ë¦½íŠ¸

set -e

echo "=========================================="
echo "Private RAG - ì´ˆê¸° ì„¤ì •"
echo "=========================================="

# ë””ë ‰í† ë¦¬ ìƒì„±
echo "ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„± ì¤‘..."
mkdir -p data/uploads
mkdir -p data/chroma_db
mkdir -p models

# Backend ì˜ì¡´ì„± ì„¤ì¹˜
echo "ğŸ“¦ Backend ì˜ì¡´ì„± ì„¤ì¹˜ ì¤‘..."
cd backend
python -m venv venv || python3 -m venv venv
source venv/bin/activate || source venv/Scripts/activate  # Windows
pip install -r requirements.txt
cd ..

# Frontend ì˜ì¡´ì„± ì„¤ì¹˜
echo "ğŸ“¦ Frontend ì˜ì¡´ì„± ì„¤ì¹˜ ì¤‘..."
cd frontend
npm install
cd ..

# ëª¨ë¸ ì´ˆê¸°í™”
echo "ğŸ¤– ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."
cd backend
python init_models.py
cd ..

# Ollama ëª¨ë¸ í™•ì¸
echo ""
echo "âš ï¸  Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:"
echo "   ollama pull llama3.1:8b-instruct-q4_K_M"

echo ""
echo "=========================================="
echo "âœ… ì„¤ì • ì™„ë£Œ!"
echo "=========================================="
echo ""
echo "ì‹¤í–‰ ë°©ë²•:"
echo "  Backend:  cd backend && python app.py"
echo "  Frontend: cd frontend && npm run dev"
echo "  Ollama:   ollama serve (ë³„ë„ í„°ë¯¸ë„)"

